#include <algorithm>
#include <cmath>
#include <cstdlib>
#include <cstring>
#include <vector>

#include "tensorflow/lite/micro/micro_interpreter.h"
#include "tensorflow/lite/micro/micro_log.h"
#include "tensorflow/lite/micro/micro_mutable_op_resolver.h"
#include "tensorflow/lite/micro/system_setup.h"
#include "tensorflow/lite/schema/schema_generated.h"

// Model data generated by quantize.py
extern const unsigned char mini_rnn_gesture_model[];
extern const unsigned int mini_rnn_gesture_model_len;

namespace {

// Constants matching the Mini-RNN architecture
const int kNumChannels = 16;
const int kSeqLen = 50;      // 25ms window @ 2kHz
const int kNumGestures = 9;  // Gesture classes 0-8
const int kHiddenSize = 128; // Stateful GRU hidden dimension
const int kStride = 20;      // 10ms inference stride
const int kSampleRate = 2000;

// Memory for TFLM interpreter
const int kTensorArenaSize = 384 * 1024;
uint8_t tensor_arena[kTensorArenaSize];

const char* kGestureNames[] = {
    "index_press", "index_release", "middle_press", "middle_release",
    "thumb_click", "thumb_down", "thumb_in", "thumb_out", "thumb_up"
};

// Normalization stats (per-channel Z-Score) aligned with Python training
const float kNormMean[16] = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0};
const float kNormStd[16] = {
    6.013952f, 5.859423f, 6.209393f, 7.271179f,
    12.334139f, 11.997432f, 4.842080f, 5.316736f,
    5.355888f, 6.679272f, 7.037768f, 8.026479f,
    11.293577f, 14.077667f, 10.762142f, 7.953421f
};

/**
 * OnlineNormalizer - Dynamic calibration using EMA
 */
class OnlineNormalizer {
 public:
  explicit OnlineNormalizer(float alpha = 0.001f, bool enabled = false)
      : alpha_(alpha), enabled_(enabled), initialized_(false) {
    std::copy(kNormMean, kNormMean + kNumChannels, running_mean_);
    std::copy(kNormStd, kNormStd + kNumChannels, running_std_);
  }

  void Normalize(float window[kSeqLen][kNumChannels]) {
    if (enabled_) {
      float batch_mean[kNumChannels] = {0};
      float batch_var[kNumChannels] = {0};

      for (int c = 0; c < kNumChannels; ++c) {
        for (int t = 0; t < kSeqLen; ++t) batch_mean[c] += window[t][c];
        batch_mean[c] /= kSeqLen;
        for (int t = 0; t < kSeqLen; ++t) {
          float diff = window[t][c] - batch_mean[c];
          batch_var[c] += diff * diff;
        }
        batch_var[c] = sqrtf(batch_var[c] / kSeqLen) + 1e-6f;
      }

      if (!initialized_) {
        std::copy(batch_mean, batch_mean + kNumChannels, running_mean_);
        std::copy(batch_var, batch_var + kNumChannels, running_std_);
        initialized_ = true;
      } else {
        for (int c = 0; c < kNumChannels; ++c) {
          running_mean_[c] = alpha_ * batch_mean[c] + (1.0f - alpha_) * running_mean_[c];
          running_std_[c] = alpha_ * batch_var[c] + (1.0f - alpha_) * running_std_[c];
        }
      }
    }

    for (int t = 0; t < kSeqLen; ++t) {
      for (int c = 0; c < kNumChannels; ++c) {
        window[t][c] = (window[t][c] - running_mean_[c]) / running_std_[c];
      }
    }
  }

 private:
  float alpha_;
  bool enabled_, initialized_;
  float running_mean_[kNumChannels], running_std_[kNumChannels];
};

/**
 * FingerStateMachine - Filters logically impossible gesture transitions
 */
class FingerStateMachine {
 public:
  enum State { NEUTRAL, INDEX, MIDDLE };
  FingerStateMachine() : state_(NEUTRAL) {}

  bool ShouldFilter(int32_t gesture_id) const {
    if (state_ == NEUTRAL) return gesture_id == 1 || gesture_id == 3;
    if (state_ == INDEX) return gesture_id == 3;
    if (state_ == MIDDLE) return gesture_id == 1;
    return false;
  }

  void Update(int32_t gesture_id) {
    if (gesture_id == 0) state_ = INDEX;
    else if (gesture_id == 2) state_ = MIDDLE;
    else if (gesture_id == 1 || gesture_id == 3) state_ = NEUTRAL;
  }

 private:
  State state_;
};

/**
 * GestureDebouncer - Energy integration and competition logic matching Python
 */
class GestureDebouncer {
 public:
  struct Config {
    float threshold_on = 0.40f;
    int lockout_frames = 20; // 200ms
    float competition_margin = 0.15f;
  };

  explicit GestureDebouncer(Config config)
      : config_(config), global_lockout_(0) {
    std::fill(energies_, energies_ + kNumGestures, 0.0f);
    std::fill(lockout_, lockout_ + kNumGestures, 0);
  }

  int32_t Process(const float* probs) {
    if (global_lockout_ > 0) global_lockout_--;
    for (int i = 0; i < kNumGestures; ++i) {
      if (lockout_[i] > 0) lockout_[i]--;
    }

    // Leaky bucket energy integration (Alpha=0.1)
    for (int i = 0; i < kNumGestures; ++i) {
      energies_[i] = energies_[i] * 0.9f + probs[i] * 0.1f;
    }

    // Find class with highest integrated energy
    int32_t best_idx = 0;
    for (int i = 1; i < kNumGestures; ++i) {
      if (energies_[i] > energies_[best_idx]) best_idx = i;
    }

    // Ambiguity check: Top-1 vs Top-2 raw probability shift
    float p1 = 0, p2 = 0;
    for (int i = 0; i < kNumGestures; ++i) {
      if (probs[i] > p1) { p2 = p1; p1 = probs[i]; }
      else if (probs[i] > p2) p2 = probs[i];
    }
    bool is_ambiguous = (p1 - p2) < config_.competition_margin;

    // Trigger if energy exceeds threshold and logic constraints are met
    if (!is_ambiguous && energies_[best_idx] > config_.threshold_on) {
      if (lockout_[best_idx] == 0 && global_lockout_ == 0) {
        lockout_[best_idx] = config_.lockout_frames;
        global_lockout_ = 5; // 50ms global lockout
        std::fill(energies_, energies_ + kNumGestures, 0.0f);
        return best_idx;
      }
    }
    return -1;
  }

 private:
  Config config_;
  float energies_[kNumGestures];
  int lockout_[kNumGestures], global_lockout_;
};

/**
 * MiniRNNHandler - Manages TFLM interpreter and stateful GRU inference
 */
class MiniRNNHandler {
 public:
  bool Initialize() {
    tflite::InitializeTarget();
    model_ = tflite::GetModel(mini_rnn_gesture_model);
    if (model_->version() != TFLITE_SCHEMA_VERSION) return false;

    static tflite::MicroMutableOpResolver<20> resolver;
    resolver.AddAdd(); resolver.AddAveragePool2D(); resolver.AddConcatenation();
    resolver.AddConv2D(); resolver.AddDequantize(); resolver.AddFullyConnected();
    resolver.AddLogistic(); resolver.AddMul(); resolver.AddReshape();
    resolver.AddTanh(); resolver.AddPad(); resolver.AddMean();
    resolver.AddSub(); resolver.AddStridedSlice(); resolver.AddSplit();
    resolver.AddNeg(); resolver.AddRsqrt(); resolver.AddUnpack();
    resolver.AddSquaredDifference(); resolver.AddQuantize();

    static tflite::MicroInterpreter interp(model_, resolver, tensor_arena, kTensorArenaSize);
    interpreter_ = &interp;
    if (interpreter_->AllocateTensors() != kTfLiteOk) return false;

    input_emg_ = interpreter_->input(0);
    input_state_ = interpreter_->input(1);
    for (size_t i = 0; i < interpreter_->outputs_size(); ++i) {
      TfLiteTensor* t = interpreter_->output(i);
      if (t->dims->size == 2) {
        if (t->dims->data[1] == kNumGestures) output_probs_ = t;
        else if (t->dims->data[1] == kHiddenSize) output_state_ = t;
      }
    }
    ResetState();
    return output_probs_ && output_state_;
  }

  int32_t RunInference(const float window[kSeqLen][kNumChannels], float* probs, bool apply_std) {
    bool is_float = (input_emg_->type == kTfLiteFloat32);
    
    // 1. Feature Engineering: [Raw, Abs]
    for (int t = 0; t < kSeqLen; ++t) {
      for (int c = 0; c < kNumChannels; ++c) {
        float val = apply_std ? (window[t][c] - kNormMean[c]) / (kNormStd[c] + 1e-6f) : window[t][c];
        int idx = t * (kNumChannels * 2) + c;
        if (is_float) {
          input_emg_->data.f[idx] = val;
          input_emg_->data.f[idx + kNumChannels] = std::abs(val);
        } else {
          auto q = [&](float v) {
            float s = input_emg_->params.scale;
            int32_t zp = input_emg_->params.zero_point;
            return static_cast<int8_t>(std::max(-128, std::min(127, (int32_t)(v / s + zp))));
          };
          input_emg_->data.int8[idx] = q(val);
          input_emg_->data.int8[idx + kNumChannels] = q(std::abs(val));
        }
      }
    }

    // 2. Set Input State
    if (is_float) std::copy(hidden_state_, hidden_state_ + kHiddenSize, input_state_->data.f);
    else {
      float s = input_state_->params.scale; int32_t zp = input_state_->params.zero_point;
      for (int i = 0; i < kHiddenSize; ++i)
        input_state_->data.int8[i] = static_cast<int8_t>(std::max(-128, std::min(127, (int32_t)(hidden_state_[i] / s + zp))));
    }

    if (interpreter_->Invoke() != kTfLiteOk) return -1;

    // 3. Extract Probs and Update Hidden State
    auto deq = [](TfLiteTensor* t, int i) {
      return (t->type == kTfLiteFloat32) ? t->data.f[i] : (t->data.int8[i] - t->params.zero_point) * t->params.scale;
    };
    for (int i = 0; i < kNumGestures; ++i) probs[i] = deq(output_probs_, i);
    for (int i = 0; i < kHiddenSize; ++i) hidden_state_[i] = deq(output_state_, i);

    return 0;
  }

  void ResetState() { std::fill(hidden_state_, hidden_state_ + kHiddenSize, 0.0f); }

 private:
  const tflite::Model* model_ = nullptr;
  tflite::MicroInterpreter* interpreter_ = nullptr;
  TfLiteTensor *input_emg_ = nullptr, *input_state_ = nullptr, *output_probs_ = nullptr, *output_state_ = nullptr;
  float hidden_state_[kHiddenSize];
};

} // namespace

int main(int argc, char* argv[]) {
  const char* bin_path = "emg_data/discrete_gestures_user_002_dataset_000.bin";
  float threshold_on = 0.40f;
  bool adaptive_norm = false;

  for (int i = 1; i < argc; ++i) {
    if (std::strcmp(argv[i], "--dataset") == 0 && i + 1 < argc) bin_path = argv[++i];
    else if (std::strcmp(argv[i], "--threshold") == 0 && i + 1 < argc) threshold_on = (float)std::atof(argv[++i]);
    else if (std::strcmp(argv[i], "--adaptive_norm") == 0) adaptive_norm = true;
  }

  MiniRNNHandler handler;
  if (!handler.Initialize()) return 1;

  GestureDebouncer debouncer({threshold_on, 20, 0.15f});
  FingerStateMachine finger_state;
  OnlineNormalizer normalizer(0.001f, adaptive_norm);

  MicroPrintf("Mini-RNN Demo (Sample-Aligned, Optimized)");
  FILE* file = std::fopen(bin_path, "rb");
  if (!file) return 1;

  struct { double ts; float emg[kNumChannels]; } sample;
  float window[kSeqLen][kNumChannels] = {0};
  float probs[kNumGestures];
  int total_samples = 0;

  while (std::fread(&sample, sizeof(sample), 1, file) == 1) {
    total_samples++;
    // Shift window sample-by-sample for perfect temporal alignment
    if (kSeqLen > 1) {
      std::memmove(window, window + 1, (kSeqLen - 1) * sizeof(window[0]));
    }
    std::memcpy(window[kSeqLen - 1], sample.emg, sizeof(sample.emg));

    // np.arange(50, len, 20) alignment
    if (total_samples >= kSeqLen && (total_samples - kSeqLen) % kStride == 0) {
      float norm_window[kSeqLen][kNumChannels];
      std::memcpy(norm_window, window, sizeof(window));
      if (adaptive_norm) normalizer.Normalize(norm_window);

      if (handler.RunInference(norm_window, probs, !adaptive_norm) == 0) {
        int32_t gid = debouncer.Process(probs);
        if (gid != -1 && !finger_state.ShouldFilter(gid)) {
          finger_state.Update(gid);
          
          // Construct Top-3 filtered log (Prob >= 0.2)
          struct Pred { int id; float p; };
          Pred ps[kNumGestures];
          for (int i = 0; i < kNumGestures; ++i) ps[i] = {i, probs[i]};
          std::sort(ps, ps + kNumGestures, [](Pred a, Pred b){ return a.p > b.p; });

          char buf[128];
          int off = snprintf(buf, sizeof(buf), "%.3f: [\342\234\224]", sample.ts);
          for (int k = 0; k < 3 && ps[k].p >= 0.20f; ++k) {
            off += snprintf(buf + off, sizeof(buf) - off, " %s/%.2f", kGestureNames[ps[k].id], (double)ps[k].p);
          }
          MicroPrintf("%s", buf);
        }
      }
    }
  }

  std::fclose(file);
  return 0;
}
