"""
æ¨¡å‹é‡åŒ–å·¥å…· - å°†FP32æ¨¡å‹è½¬ä¸ºINT8

INT8é‡åŒ–å¯ä»¥ï¼š
1. æ¨¡å‹å¤§å°ç¼©å°4å€
2. æ¨ç†é€Ÿåº¦æå‡2-4å€
3. å†…å­˜å ç”¨å‡å°‘75%

é€‚ç”¨äºESP32ç­‰èµ„æºå—é™è®¾å¤‡
"""

import torch
import torch.quantization as quantization
from pathlib import Path
import sys

sys.path.append(str(Path(__file__).parent.parent.parent.parent.parent))


def quantize_model_dynamic(
    model: torch.nn.Module,
    output_path: str,
    qconfig_spec: set = None
) -> torch.nn.Module:
    """
    åŠ¨æ€é‡åŒ–ï¼ˆæ¨èç”¨äºLSTM/GRUæ¨¡å‹ï¼‰
    
    åªé‡åŒ–æƒé‡ï¼Œæ¿€æ´»å€¼åœ¨è¿è¡Œæ—¶åŠ¨æ€é‡åŒ–
    
    Parameters
    ----------
    model : torch.nn.Module
        å¾…é‡åŒ–çš„FP32æ¨¡å‹
    output_path : str
        é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„
    qconfig_spec : set
        è¦é‡åŒ–çš„å±‚ç±»å‹ï¼Œé»˜è®¤{nn.Linear, nn.LSTM, nn.GRU}
    
    Returns
    -------
    quantized_model : torch.nn.Module
        é‡åŒ–åçš„æ¨¡å‹
    """
    if qconfig_spec is None:
        qconfig_spec = {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU}
    
    model.eval()
    
    # åŠ¨æ€é‡åŒ–
    quantized_model = quantization.quantize_dynamic(
        model,
        qconfig_spec=qconfig_spec,
        dtype=torch.qint8
    )
    
    # ä¿å­˜
    torch.save(quantized_model.state_dict(), output_path)
    
    print(f"âœ… åŠ¨æ€é‡åŒ–å®Œæˆ: {output_path}")
    
    return quantized_model


def quantize_model_static(
    model: torch.nn.Module,
    calibration_data: torch.Tensor,
    output_path: str
) -> torch.nn.Module:
    """
    é™æ€é‡åŒ–ï¼ˆæ¨èç”¨äºçº¯å·ç§¯æ¨¡å‹ï¼‰
    
    æƒé‡å’Œæ¿€æ´»å€¼éƒ½é¢„å…ˆé‡åŒ–ï¼Œç²¾åº¦æœ€é«˜
    
    Parameters
    ----------
    model : torch.nn.Module
        å¾…é‡åŒ–çš„FP32æ¨¡å‹
    calibration_data : torch.Tensor
        æ ¡å‡†æ•°æ®ï¼Œç”¨äºç»Ÿè®¡æ¿€æ´»å€¼èŒƒå›´
    output_path : str
        é‡åŒ–åæ¨¡å‹ä¿å­˜è·¯å¾„
    
    Returns
    -------
    quantized_model : torch.nn.Module
        é‡åŒ–åçš„æ¨¡å‹
    """
    model.eval()
    
    # è®¾ç½®é‡åŒ–é…ç½®
    model.qconfig = quantization.get_default_qconfig('fbgemm')
    
    # å‡†å¤‡é‡åŒ–
    quantization.prepare(model, inplace=True)
    
    # æ ¡å‡†ï¼ˆç”¨çœŸå®æ•°æ®è¿è¡Œä¸€æ¬¡ï¼‰
    with torch.no_grad():
        _ = model(calibration_data)
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quantization.convert(model, inplace=True)
    
    # ä¿å­˜
    torch.save(model.state_dict(), output_path)
    
    print(f"âœ… é™æ€é‡åŒ–å®Œæˆ: {output_path}")
    
    return model


def compare_model_size(
    fp32_model: torch.nn.Module,
    quantized_model: torch.nn.Module
) -> dict:
    """å¯¹æ¯”é‡åŒ–å‰åçš„æ¨¡å‹å¤§å°"""
    
    def get_model_size(model):
        torch.save(model.state_dict(), "/tmp/temp_model.pt")
        size = Path("/tmp/temp_model.pt").stat().st_size
        Path("/tmp/temp_model.pt").unlink()
        return size
    
    fp32_size = get_model_size(fp32_model)
    quantized_size = get_model_size(quantized_model)
    
    return {
        "fp32_size_kb": fp32_size / 1024,
        "quantized_size_kb": quantized_size / 1024,
        "compression_ratio": fp32_size / quantized_size,
    }


def export_to_onnx_int8(
    model: torch.nn.Module,
    output_path: str,
    input_shape: tuple = (1, 16, 2000)
):
    """
    å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼ˆINT8ï¼‰
    
    ONNXå¯ä»¥è¢«å¤šç§æ¨ç†å¼•æ“ä½¿ç”¨ï¼š
    - ONNX Runtime
    - TensorRT
    - TFLite (éœ€è¦è¿›ä¸€æ­¥è½¬æ¢)
    """
    model.eval()
    dummy_input = torch.randn(input_shape)
    
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=13,
        do_constant_folding=True,
        input_names=['emg_input'],
        output_names=['gesture_logits'],
        dynamic_axes={
            'emg_input': {2: 'sequence_length'},
            'gesture_logits': {2: 'output_length'}
        }
    )
    
    print(f"âœ… ONNXå¯¼å‡ºå®Œæˆ: {output_path}")


if __name__ == "__main__":
    print("="*60)
    print("æ¨¡å‹é‡åŒ–ç¤ºä¾‹")
    print("="*60)
    
    # 1. å¯¼å…¥æ¨¡å‹
    from ultra_light_models import (
        TinyStudentArchitecture,
        ConvOnlyStudentArchitecture,
        count_parameters
    )
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\nåˆ›å»ºConvOnlyæ¨¡å‹ï¼ˆæœ€é€‚åˆé‡åŒ–ï¼‰...")
    model = ConvOnlyStudentArchitecture()
    model.eval()
    
    # 3. åŠ¨æ€é‡åŒ–
    print("\næ‰§è¡ŒåŠ¨æ€é‡åŒ–...")
    quantized_model = quantize_model_dynamic(
        model,
        output_path="convonly_int8.pt",
        qconfig_spec={torch.nn.Conv1d, torch.nn.Linear}
    )
    
    # 4. å¯¹æ¯”å¤§å°
    print("\nğŸ“Š æ¨¡å‹å¤§å°å¯¹æ¯”:")
    size_info = compare_model_size(model, quantized_model)
    
    print(f"   FP32æ¨¡å‹: {size_info['fp32_size_kb']:.1f} KB")
    print(f"   INT8æ¨¡å‹: {size_info['quantized_size_kb']:.1f} KB")
    print(f"   å‹ç¼©æ¯”: {size_info['compression_ratio']:.2f}x")
    
    # 5. æµ‹è¯•ç²¾åº¦æŸå¤±
    print("\nğŸ§ª ç²¾åº¦æµ‹è¯•:")
    dummy_input = torch.randn(1, 16, 2000)
    
    with torch.no_grad():
        fp32_output = model(dummy_input)
        int8_output = quantized_model(dummy_input)
    
    # è®¡ç®—è¾“å‡ºå·®å¼‚
    mae = torch.mean(torch.abs(fp32_output - int8_output)).item()
    max_diff = torch.max(torch.abs(fp32_output - int8_output)).item()
    
    print(f"   å¹³å‡ç»å¯¹è¯¯å·®: {mae:.6f}")
    print(f"   æœ€å¤§è¯¯å·®: {max_diff:.6f}")
    print(f"   ç›¸å¯¹è¯¯å·®: {mae/torch.mean(torch.abs(fp32_output)).item():.2%}")
    
    # 6. å¯¼å‡ºONNX
    print("\nğŸ“¦ å¯¼å‡ºONNXæ ¼å¼...")
    export_to_onnx_int8(quantized_model, "convonly_int8.onnx")
    
    print("\nâœ¨ é‡åŒ–å®Œæˆï¼")
    print("\nä¸‹ä¸€æ­¥:")
    print("   1. ç”¨ONNXè½¬æ¢ä¸ºTFLite: onnx2tf convonly_int8.onnx")
    print("   2. æˆ–ç›´æ¥åœ¨PyTorch Mobileä¸Šè¿è¡Œ")
    print("   3. ESP32éƒ¨ç½²éœ€è¦è½¬ä¸ºTFLite Microæ ¼å¼")
